[
  {
    "objectID": "mgta495hw1.html",
    "href": "mgta495hw1.html",
    "title": "Charitable Contribution Made",
    "section": "",
    "text": "pip install pandas pyreadstat\n\nRequirement already satisfied: pandas in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (2.2.3)\nRequirement already satisfied: pyreadstat in /opt/conda/lib/python3.11/site-packages (1.2.8)\nRequirement already satisfied: numpy&gt;=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nimport pandas as pd\n\n# Load the Stata file\nfile_path = \"karlan_list_2007.dta\"\ndf = pd.read_stata(file_path)\n\n# Display basic information\nprint(\"Dataset Info:\")\nprint(df.info())\n\nDataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n# Show first few rows\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\n\n\nFirst 5 rows:\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n# Show summary statistics\nprint(\"\\nSummary statistics:\")\nprint(df.describe(include='all'))\n\n\nSummary statistics:\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168560      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378105     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258633  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\nfrom scipy.stats import ttest_ind\n\n%pip install statsmodels\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nRequirement already satisfied: statsmodels in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (0.14.4)\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.26.4)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.12.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (from statsmodels) (2.2.3)\nRequirement already satisfied: patsy&gt;=0.5.6 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: packaging&gt;=21.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (24.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from patsy&gt;=0.5.6-&gt;statsmodels) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n# Add an intercept column to the DataFrame\ndf['intercept'] = 1\n\n# Ensure consistent data for both t-test and regression\nconsistent_data = df[['mrm2', 'treatment', 'control', 'intercept']].dropna()\n\n# Perform a t-test\ntreatment_group = consistent_data[consistent_data['treatment'] == 1]['mrm2']\ncontrol_group = consistent_data[consistent_data['control'] == 1]['mrm2']\nt_stat, p_value_ttest = ttest_ind(treatment_group, control_group, equal_var=False)\n\nprint(f\"T-test results:\")\nprint(f\"T-statistic: {t_stat:.4f}, P-value: {p_value_ttest:.4f}\")\n\n# Perform a linear regression\nmodel = sm.OLS(consistent_data['mrm2'], consistent_data[['intercept', 'treatment']])\nresults = model.fit()\n\nprint(\"\\nLinear Regression results:\")\nprint(results.summary())\n\n# Confirm the p-value matches\np_value_regression = results.pvalues['treatment']\nprint(f\"\\nP-value from regression: {p_value_regression:.4f}\")\nassert abs(p_value_ttest - p_value_regression) &lt; 1e-3, \"P-values do not match!\"\n\nT-test results:\nT-statistic: 0.1195, P-value: 0.9049\n\nLinear Regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        22:50:37   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nP-value from regression: 0.9049\nimport matplotlib.pyplot as plt\n\n# Calculate the proportion of people who donated in each group\ntreatment_proportion = df[df['treatment'] == 1]['gave'].mean()\ncontrol_proportion = df[df['control'] == 1]['gave'].mean()\n\n# Create the barplot\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['blue', 'orange'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\n# Display the proportions\nprint(f\"Proportion of people who donated in the Treatment group: {treatment_proportion:.2%}\")\nprint(f\"Proportion of people who donated in the Control group: {control_proportion:.2%}\")\n\nplt.show()\n\nProportion of people who donated in the Treatment group: 2.20%\nProportion of people who donated in the Control group: 1.79%\n# Extract the binary outcome for treatment and control groups\ntreatment_gave = df[df['treatment'] == 1]['gave'].dropna()\ncontrol_gave = df[df['control'] == 1]['gave'].dropna()\n\n# Perform a t-test\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\n\nprint(f\"T-test results for charitable donation (gave):\")\nprint(f\"T-statistic: {t_stat_gave:.4f}, P-value: {p_value_gave:.4f}\")\n\nT-test results for charitable donation (gave):\nT-statistic: 3.2095, P-value: 0.0013\n# Perform a bivariate linear regression\nbivariate_model = sm.OLS(df['gave'], df[['intercept', 'treatment']])\nbivariate_results = bivariate_model.fit()\n\nprint(\"\\nBivariate Linear Regression results:\")\nprint(bivariate_results.summary())\n\n# Extract the p-value for the treatment variable\nbivariate_p_value = bivariate_results.pvalues['treatment']\nprint(f\"\\nP-value for treatment in bivariate regression: {bivariate_p_value:.4f}\")\n\n\nBivariate Linear Regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        22:50:38   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nP-value for treatment in bivariate regression: 0.0019\n# Perform a probit regression\nprobit_model = smf.probit('gave ~ treatment', data=df)\nprobit_results = probit_model.fit()\n\n# Display the summary of the probit regression\nprint(\"\\nProbit Regression results:\")\nprint(probit_results.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\nProbit Regression results:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        22:50:39   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n=============================================================================="
  },
  {
    "objectID": "mgta495hw1.html#simulation-experiment",
    "href": "mgta495hw1.html#simulation-experiment",
    "title": "Charitable Contribution Made",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# Parameters\np_no_match = 0.018  # Probability of donation without match\np_with_match = 0.022  # Probability of donation with match\nsample_sizes = [10, 50, 100, 500, 1000, 5000, 10000]  # Different sample sizes\nnum_simulations = 1000  # Number of simulations for CLT\n\n# Law of Large Numbers (LLN)\nmeans_no_match = []\nmeans_with_match = []\n\nfor size in sample_sizes:\n    sample_no_match = np.random.binomial(1, p_no_match, size)\n    sample_with_match = np.random.binomial(1, p_with_match, size)\n    means_no_match.append(np.mean(sample_no_match))\n    means_with_match.append(np.mean(sample_with_match))\n\n# Plot LLN\nplt.figure(figsize=(12, 6))\nplt.plot(sample_sizes, means_no_match, label=\"No Match (p=0.018)\", marker='o')\nplt.plot(sample_sizes, means_with_match, label=\"With Match (p=0.022)\", marker='o')\nplt.axhline(y=p_no_match, color='blue', linestyle='--', label=\"True Mean (No Match)\")\nplt.axhline(y=p_with_match, color='orange', linestyle='--', label=\"True Mean (With Match)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Sample Mean\")\nplt.title(\"Law of Large Numbers\")\nplt.legend()\nplt.grid()\nplt.show()\n\n# Central Limit Theorem (CLT)\nsample_means_no_match = []\nsample_means_with_match = []\n\nfor _ in range(num_simulations):\n    sample_no_match = np.random.binomial(1, p_no_match, 1000)  # Fixed sample size\n    sample_with_match = np.random.binomial(1, p_with_match, 1000)\n    sample_means_no_match.append(np.mean(sample_no_match))\n    sample_means_with_match.append(np.mean(sample_with_match))\n\n# Plot CLT\nplt.figure(figsize=(12, 6))\nplt.hist(sample_means_no_match, bins=30, alpha=0.7, label=\"No Match (p=0.018)\", color='blue', edgecolor='black')\nplt.hist(sample_means_with_match, bins=30, alpha=0.7, label=\"With Match (p=0.022)\", color='orange', edgecolor='black')\nplt.axvline(x=p_no_match, color='blue', linestyle='--', label=\"True Mean (No Match)\")\nplt.axvline(x=p_with_match, color='orange', linestyle='--', label=\"True Mean (With Match)\")\nplt.xlabel(\"Sample Mean\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Central Limit Theorem\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers\n\nimport numpy as np\n\n# Filter the control group\ncontrol_amounts = df[df['control'] == 1]['amount']\n\n# Simulate 100,000 draws from the control distribution\nsimulated_draws = np.random.choice(control_amounts, size=100000, replace=True)\n\n# Display summary statistics of the simulated draws\nprint(f\"Simulated Draws Summary:\")\nprint(f\"Mean: {np.mean(simulated_draws):.4f}\")\nprint(f\"Standard Deviation: {np.std(simulated_draws):.4f}\")\nprint(f\"Min: {np.min(simulated_draws):.4f}\")\nprint(f\"Max: {np.max(simulated_draws):.4f}\")\n\nSimulated Draws Summary:\nMean: 0.8120\nStandard Deviation: 8.2383\nMin: 0.0000\nMax: 300.0000\n\n\n\n# Filter the treatment group\ntreatment_amounts = df[df['treatment'] == 1]['amount']\n\n# Simulate 10,000 draws from the treatment distribution\nsimulated_treatment_draws = np.random.choice(treatment_amounts, size=10000, replace=True)\n\n# Display summary statistics of the simulated treatment draws\nprint(f\"Simulated Treatment Draws Summary:\")\nprint(f\"Mean: {np.mean(simulated_treatment_draws):.4f}\")\nprint(f\"Standard Deviation: {np.std(simulated_treatment_draws):.4f}\")\nprint(f\"Min: {np.min(simulated_treatment_draws):.4f}\")\nprint(f\"Max: {np.max(simulated_treatment_draws):.4f}\")\n\nSimulated Treatment Draws Summary:\nMean: 0.8502\nStandard Deviation: 8.7401\nMin: 0.0000\nMax: 400.0000\n\n\n\n# Simulate 100,000 draws from the control distribution\nsimulated_control_draws = np.random.choice(control_amounts, size=100000, replace=True)\n\n# Simulate 10,000 draws from the treatment distribution\nsimulated_treatment_draws = np.random.choice(treatment_amounts, size=10000, replace=True)\n\n# Calculate a vector of 10,000 differences\ndifferences = simulated_treatment_draws - simulated_control_draws[:10000]\n\n# Calculate the cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative Average of Differences\")\nplt.axhline(0, color='red', linestyle='--', label=\"Zero Line\")\nplt.xlabel(\"Number of Differences\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Cumulative Average of Differences Between Treatment and Control\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\n# Add a distribution curve to the plot\n# Fit a normal distribution to the differences\nmean_diff = np.mean(differences)\nstd_diff = np.std(differences)\n\n# Generate x values for the curve\nx = np.linspace(min(differences), max(differences), 1000)\ny = norm.pdf(x, mean_diff, std_diff)\n\n# Plot the distribution curve\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=\"Normal Distribution Curve\", color='purple')\nplt.hist(differences, bins=50, density=True, alpha=0.6, color='gray', edgecolor='black')\nplt.axvline(0, color='red', linestyle='--', label=\"Zero Line\")\nplt.xlabel(\"Differences\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of Differences with Fitted Curve\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n# Set the sample size and number of repetitions\nsample_size = 500\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 500')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the sample size and number of repetitions\nsample_size = 50\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 50')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the sample size and number of repetitions\nsample_size = 1000\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 1000')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Homework",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nYour Name\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/homework1/hw1_questions.html",
    "href": "projects/homework1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results.\nIn an effort to rigorously test fundraising strategies, Karlan and List conducted a large-scale natural field experiment involving over 50,000 prior donors to a politically oriented nonprofit organization. The central objective was to investigate whether and how price-like mechanisms—specifically matching grants—affect charitable giving behavior."
  },
  {
    "objectID": "projects/homework1/hw1_questions.html#introduction",
    "href": "projects/homework1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results.\nIn an effort to rigorously test fundraising strategies, Karlan and List conducted a large-scale natural field experiment involving over 50,000 prior donors to a politically oriented nonprofit organization. The central objective was to investigate whether and how price-like mechanisms—specifically matching grants—affect charitable giving behavior."
  },
  {
    "objectID": "projects/homework1/hw1_questions.html#data",
    "href": "projects/homework1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport sys\nprint(sys.version)\n\n3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:25:01) [GCC 12.3.0]\n\n\n\nimport pandas as pd\n\n# Load the Stata file\nfile_path = \"/home/jovyan/Desktop/marketingwebsite/karlan_list_2007.dta\"\ndf = pd.read_stata(file_path)\n\n# Display basic information\nprint(\"Dataset Info:\")\nprint(df.info())\n\nDataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\n\n\n# Show summary statistics\nprint(\"\\nSummary statistics:\")\nprint(df.describe(include='all'))\n\n\nSummary statistics:\n           treatment       control    ratio        ratio2        ratio3  \\\ncount   50083.000000  50083.000000    50083  50083.000000  50083.000000   \nunique           NaN           NaN        4           NaN           NaN   \ntop              NaN           NaN  Control           NaN           NaN   \nfreq             NaN           NaN    16687           NaN           NaN   \nmean        0.666813      0.333187      NaN      0.222311      0.222211   \nstd         0.471357      0.471357      NaN      0.415803      0.415736   \nmin         0.000000      0.000000      NaN      0.000000      0.000000   \n25%         0.000000      0.000000      NaN      0.000000      0.000000   \n50%         1.000000      0.000000      NaN      0.000000      0.000000   \n75%         1.000000      1.000000      NaN      0.000000      0.000000   \nmax         1.000000      1.000000      NaN      1.000000      1.000000   \n\n           size        size25        size50       size100        sizeno  ...  \\\ncount     50083  50083.000000  50083.000000  50083.000000  50083.000000  ...   \nunique        5           NaN           NaN           NaN           NaN  ...   \ntop     Control           NaN           NaN           NaN           NaN  ...   \nfreq      16687           NaN           NaN           NaN           NaN  ...   \nmean        NaN      0.166723      0.166623      0.166723      0.166743  ...   \nstd         NaN      0.372732      0.372643      0.372732      0.372750  ...   \nmin         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n25%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n50%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \n75%         NaN      0.000000      0.000000      0.000000      0.000000  ...   \nmax         NaN      1.000000      1.000000      1.000000      1.000000  ...   \n\n              redcty       bluecty        pwhite        pblack     page18_39  \\\ncount   49978.000000  49978.000000  48217.000000  48047.000000  48217.000000   \nunique           NaN           NaN           NaN           NaN           NaN   \ntop              NaN           NaN           NaN           NaN           NaN   \nfreq             NaN           NaN           NaN           NaN           NaN   \nmean        0.510245      0.488715      0.819599      0.086710      0.321694   \nstd         0.499900      0.499878      0.168560      0.135868      0.103039   \nmin         0.000000      0.000000      0.009418      0.000000      0.000000   \n25%         0.000000      0.000000      0.755845      0.014729      0.258311   \n50%         1.000000      0.000000      0.872797      0.036554      0.305534   \n75%         1.000000      1.000000      0.938827      0.090882      0.369132   \nmax         1.000000      1.000000      1.000000      0.989622      0.997544   \n\n           ave_hh_sz  median_hhincome        powner  psch_atlstba  \\\ncount   48221.000000     48209.000000  48214.000000  48215.000000   \nunique           NaN              NaN           NaN           NaN   \ntop              NaN              NaN           NaN           NaN   \nfreq             NaN              NaN           NaN           NaN   \nmean        2.429012     54815.700533      0.669418      0.391661   \nstd         0.378105     22027.316665      0.193405      0.186599   \nmin         0.000000      5000.000000      0.000000      0.000000   \n25%         2.210000     39181.000000      0.560222      0.235647   \n50%         2.440000     50673.000000      0.712296      0.373744   \n75%         2.660000     66005.000000      0.816798      0.530036   \nmax         5.270000    200001.000000      1.000000      1.000000   \n\n        pop_propurban  \ncount    48217.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.871968  \nstd          0.258633  \nmin          0.000000  \n25%          0.884929  \n50%          1.000000  \n75%          1.000000  \nmax          1.000000  \n\n[11 rows x 51 columns]\n\n\nSome observations from the summary statistics: - The treatment variable has a mean of 0.666813, indicating about two-thirds of the sample received a treatment letter.\n\nThe control variable has a mean of 0.333187, indicating one-third of the sample received the standard letter with no match.\nThe ratio2 (2:1 match) and ratio3 (3:1 match) indicators each have a mean of approximately 0.222, meaning about 22% of the sample received these specific matching offers.\nThe size25, size50, size100, and sizeno match threshold treatments each have means around 0.167, showing equal distribution across these four match threshold categories.\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nfrom scipy.stats import ttest_ind\n\n%pip install statsmodels\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nRequirement already satisfied: statsmodels in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (0.14.4)\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.26.4)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.12.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (from statsmodels) (2.2.3)\nRequirement already satisfied: patsy&gt;=0.5.6 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: packaging&gt;=21.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (24.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from patsy&gt;=0.5.6-&gt;statsmodels) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# Add an intercept column to the DataFrame\ndf['intercept'] = 1\n\n# Ensure consistent data for both t-test and regression\nconsistent_data = df[['mrm2', 'treatment', 'control', 'intercept']].dropna()\n\n# Perform a t-test\ntreatment_group = consistent_data[consistent_data['treatment'] == 1]['mrm2']\ncontrol_group = consistent_data[consistent_data['control'] == 1]['mrm2']\nt_stat, p_value_ttest = ttest_ind(treatment_group, control_group, equal_var=False)\n\nprint(f\"T-test results:\")\nprint(f\"T-statistic: {t_stat:.4f}, P-value: {p_value_ttest:.4f}\")\n\n# Perform a linear regression\nmodel = sm.OLS(consistent_data['mrm2'], consistent_data[['intercept', 'treatment']])\nresults = model.fit()\n\nprint(\"\\nLinear Regression results:\")\nprint(results.summary())\n\n# Confirm the p-value matches\np_value_regression = results.pvalues['treatment']\nprint(f\"\\nP-value from regression: {p_value_regression:.4f}\")\nassert abs(p_value_ttest - p_value_regression) &lt; 1e-3, \"P-values do not match!\"\n\nT-test results:\nT-statistic: 0.1195, P-value: 0.9049\n\nLinear Regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\nTime:                        23:56:35   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nP-value from regression: 0.9049"
  },
  {
    "objectID": "projects/homework1/hw1_questions.html#experimental-results",
    "href": "projects/homework1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n# Calculate the proportion of people who donated in each group\ntreatment_proportion = df[df['treatment'] == 1]['gave'].mean()\ncontrol_proportion = df[df['control'] == 1]['gave'].mean()\n\n# Create the barplot\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['blue', 'orange'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\n# Display the proportions\nprint(f\"Proportion of people who donated in the Treatment group: {treatment_proportion:.2%}\")\nprint(f\"Proportion of people who donated in the Control group: {control_proportion:.2%}\")\n\nplt.show()\n\nProportion of people who donated in the Treatment group: 2.20%\nProportion of people who donated in the Control group: 1.79%\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\n%pip install statsmodels\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nRequirement already satisfied: statsmodels in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (0.14.4)\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.26.4)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (1.12.0)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /home/jovyan/.rsm-msba/lib/python3.11/site-packages (from statsmodels) (2.2.3)\nRequirement already satisfied: patsy&gt;=0.5.6 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: packaging&gt;=21.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels) (24.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2024.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from patsy&gt;=0.5.6-&gt;statsmodels) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport statsmodels.api as sm\n\n# Add an intercept column to the DataFrame\ndf['intercept'] = 1\n\n# Perform a bivariate linear regression\nbivariate_model = sm.OLS(df['gave'], df[['intercept', 'treatment']])\nbivariate_results = bivariate_model.fit()\n\nprint(\"\\nBivariate Linear Regression results:\")\nprint(bivariate_results.summary())\n\n# Extract the p-value for the treatment variable\nbivariate_p_value = bivariate_results.pvalues['treatment']\nprint(f\"\\nP-value for treatment in bivariate regression: {bivariate_p_value:.4f}\")\n\n\nBivariate Linear Regression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        23:56:37   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nP-value for treatment in bivariate regression: 0.0019\n\n\n\n# Perform a probit regression\nprobit_model = smf.probit('gave ~ treatment', data=df)\nprobit_results = probit_model.fit()\n\n# Display the summary of the probit regression\nprint(\"\\nProbit Regression results:\")\nprint(probit_results.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\nProbit Regression results:\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        23:56:38   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nNotes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nP-value for treatment in bivariate regression: 0.0019\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Extract unique match ratios\nmatch_ratios = df['ratio'].unique()\n\n# Perform pairwise t-tests between match ratios\nfor i in range(len(match_ratios)):\n    for j in range(i + 1, len(match_ratios)):\n        ratio1 = match_ratios[i]\n        ratio2 = match_ratios[j]\n        \n        # Filter data for the two match ratios\n        group1 = df[df['ratio'] == ratio1]['gave'].dropna()\n        group2 = df[df['ratio'] == ratio2]['gave'].dropna()\n        \n        # Perform t-test\n        t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n        \n        # Print results\n        print(f\"T-test between match ratios {ratio1} and {ratio2}:\")\n        print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\\n\")\n\nT-test between match ratios Control and 1:\nT-statistic: -1.7046, P-value: 0.0883\n\nT-test between match ratios Control and 2:\nT-statistic: -2.7396, P-value: 0.0062\n\nT-test between match ratios Control and 3:\nT-statistic: -2.7926, P-value: 0.0052\n\nT-test between match ratios 1 and 2:\nT-statistic: -0.9650, P-value: 0.3345\n\nT-test between match ratios 1 and 3:\nT-statistic: -1.0150, P-value: 0.3101\n\nT-test between match ratios 2 and 3:\nT-statistic: -0.0501, P-value: 0.9600\n\n\n\n\n# Create the variable `ratio1` from the `ratio` column\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Perform the regression\nregression_model = sm.OLS(df['gave'], df[['intercept', 'ratio1', 'ratio2', 'ratio3']])\nregression_results = regression_model.fit()\n\n# Display the regression results\nprint(\"\\nRegression results:\")\nprint(regression_results.summary())\n\n\nRegression results:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        23:56:38   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Directly from the data\nresponse_rate_1_1 = df[df['ratio'] == 1]['gave'].mean()\nresponse_rate_2_1 = df[df['ratio'] == 2]['gave'].mean()\nresponse_rate_3_1 = df[df['ratio'] == 3]['gave'].mean()\n\ndifference_1_1_2_1 = response_rate_1_1 - response_rate_2_1\ndifference_2_1_3_1 = response_rate_2_1 - response_rate_3_1\n\nprint(f\"Response rate difference (1:1 - 2:1): {difference_1_1_2_1:.4f}\")\nprint(f\"Response rate difference (2:1 - 3:1): {difference_2_1_3_1:.4f}\")\n\n# Using fitted coefficients from the regression\ncoefficients = regression_results.params\ndifference_1_1_2_1_coeff = coefficients[1]  # Coefficient for ratio 2\ndifference_2_1_3_1_coeff = coefficients[2] - coefficients[1]  # Difference between coefficients for ratio 3 and ratio 2\n\nprint(f\"Response rate difference from coefficients (1:1 - 2:1): {difference_1_1_2_1_coeff:.4f}\")\nprint(f\"Response rate difference from coefficients (2:1 - 3:1): {difference_2_1_3_1_coeff:.4f}\")\n\nResponse rate difference (1:1 - 2:1): -0.0019\nResponse rate difference (2:1 - 3:1): -0.0001\nResponse rate difference from coefficients (1:1 - 2:1): 0.0029\nResponse rate difference from coefficients (2:1 - 3:1): 0.0019\n\n\n/tmp/ipykernel_59360/298916022.py:14: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n/tmp/ipykernel_59360/298916022.py:15: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\nIncreasing donation match ratios from 1:1 to 2:1 slightly boosts response rates, but moving from 2:1 to 3:1 offers minimal additional impact. Response rates rise from 2.07% (1:1) to 2.26% (2:1) and only slightly to 2.27% (3:1), with diminishing returns evident in both response differences and regression coefficients. This suggests that while a moderate increase in match size can be effective, higher ratios beyond 2:1 may not significantly enhance donor motivation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Perform a bivariate linear regression of donation amount on treatment status\nbivariate_amount_model = sm.OLS(df['amount'], df[['intercept', 'treatment']])\nbivariate_amount_results = bivariate_amount_model.fit()\n\n# Display the regression results\nprint(\"\\nBivariate Linear Regression results for donation amount on treatment status:\")\nprint(bivariate_amount_results.summary())\n\n\nBivariate Linear Regression results for donation amount on treatment status:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        23:56:38   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nObservations:\nEffect of Treatment on Donation Amount: The coefficient for the treatment variable is 0.1536, indicating that being in the treatment group (e.g., exposed to a donation match offer) is associated with an increase of approximately $0.15 in the average donation amount. However, this effect is not statistically significant at the conventional 5% level (p = 0.063), suggesting we cannot confidently conclude that treatment has a real impact on donation amount.\nModel Fit: The R-squared value is 0.000, indicating that the model explains virtually none of the variation in donation amount. This is common in social science data but implies that other factors (beyond just treatment status) are likely influencing donation amounts.\nStatistical Significance: While the p-value for the treatment variable (0.063) is close to 0.05, it slightly exceeds it, meaning the result is only marginally significant. This suggests a possible, but weak, effect of treatment on donation amount that might warrant further investigation with a larger sample or additional controls.\nBaseline Donation Amount: The intercept is 0.8133, indicating that individuals in the control group donated around $0.81 on average.\n\n# Filter the data to include only people who made a donation\ndonors_df = df[df['gave'] == 1]\n\n# Perform a bivariate linear regression of donation amount on treatment status for donors\nbivariate_donors_model = sm.OLS(donors_df['amount'], donors_df[['intercept', 'treatment']])\nbivariate_donors_results = bivariate_donors_model.fit()\n\n# Display the regression results\nprint(\"\\nBivariate Linear Regression results for donation amount on treatment status (donors only):\")\nprint(bivariate_donors_results.summary())\n\n\nBivariate Linear Regression results for donation amount on treatment status (donors only):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        23:56:38   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nInterpretations:\n\nDonors in the treatment group gave about $1.67 less on average than those in the control group. However, this difference is not statistically significant (p-value = 0.561), meaning we can’t confidently say it’s a real effect.\nThere’s no strong evidence that treatment affects the amount donated, conditional on donating.\nThe treatment coefficient should not be interpreted causally because the sample is restricted to donors only.\nTreatment could influence both whether someone donates and how much they give. By only analyzing donors, we’re conditioning on a post-treatment outcome, which can lead to biased estimates (known as collider bias).\n\n\nimport matplotlib.pyplot as plt\n\n# Filter donation amounts for treatment and control groups among donors\ntreatment_donations = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_donations = donors_df[donors_df['control'] == 1]['amount']\n\n# Calculate the sample averages\ntreatment_avg = treatment_donations.mean()\ncontrol_avg = control_donations.mean()\n\n# Create the histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donations, bins=30, color='blue', alpha=0.7, edgecolor='black')\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donations, bins=30, color='green', alpha=0.7, edgecolor='black')\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\n# Display the plots\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/homework1/hw1_questions.html#simulation-experiment",
    "href": "projects/homework1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# Parameters\np_no_match = 0.018  # Probability of donation without match\np_with_match = 0.022  # Probability of donation with match\nsample_sizes = [10, 50, 100, 500, 1000, 5000, 10000]  # Different sample sizes\nnum_simulations = 1000  # Number of simulations for CLT\n\n# Law of Large Numbers (LLN)\nmeans_no_match = []\nmeans_with_match = []\n\nfor size in sample_sizes:\n    sample_no_match = np.random.binomial(1, p_no_match, size)\n    sample_with_match = np.random.binomial(1, p_with_match, size)\n    means_no_match.append(np.mean(sample_no_match))\n    means_with_match.append(np.mean(sample_with_match))\n\n# Plot LLN\nplt.figure(figsize=(12, 6))\nplt.plot(sample_sizes, means_no_match, label=\"No Match (p=0.018)\", marker='o')\nplt.plot(sample_sizes, means_with_match, label=\"With Match (p=0.022)\", marker='o')\nplt.axhline(y=p_no_match, color='blue', linestyle='--', label=\"True Mean (No Match)\")\nplt.axhline(y=p_with_match, color='orange', linestyle='--', label=\"True Mean (With Match)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Sample Mean\")\nplt.title(\"Law of Large Numbers\")\nplt.legend()\nplt.grid()\nplt.show()\n\n# Central Limit Theorem (CLT)\nsample_means_no_match = []\nsample_means_with_match = []\n\nfor _ in range(num_simulations):\n    sample_no_match = np.random.binomial(1, p_no_match, 1000)  # Fixed sample size\n    sample_with_match = np.random.binomial(1, p_with_match, 1000)\n    sample_means_no_match.append(np.mean(sample_no_match))\n    sample_means_with_match.append(np.mean(sample_with_match))\n\n# Plot CLT\nplt.figure(figsize=(12, 6))\nplt.hist(sample_means_no_match, bins=30, alpha=0.7, label=\"No Match (p=0.018)\", color='blue', edgecolor='black')\nplt.hist(sample_means_with_match, bins=30, alpha=0.7, label=\"With Match (p=0.022)\", color='orange', edgecolor='black')\nplt.axvline(x=p_no_match, color='blue', linestyle='--', label=\"True Mean (No Match)\")\nplt.axvline(x=p_with_match, color='orange', linestyle='--', label=\"True Mean (With Match)\")\nplt.xlabel(\"Sample Mean\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Central Limit Theorem\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaw of Large Numbers\n\nimport numpy as np\n\n# Filter the control group\ncontrol_amounts = df[df['control'] == 1]['amount']\n\n# Simulate 100,000 draws from the control distribution\nsimulated_draws = np.random.choice(control_amounts, size=100000, replace=True)\n\n# Display summary statistics of the simulated draws\nprint(f\"Simulated Draws Summary:\")\nprint(f\"Mean: {np.mean(simulated_draws):.4f}\")\nprint(f\"Standard Deviation: {np.std(simulated_draws):.4f}\")\nprint(f\"Min: {np.min(simulated_draws):.4f}\")\nprint(f\"Max: {np.max(simulated_draws):.4f}\")\n\nSimulated Draws Summary:\nMean: 0.8450\nStandard Deviation: 8.3746\nMin: 0.0000\nMax: 300.0000\n\n\n\n# Filter the treatment group\ntreatment_amounts = df[df['treatment'] == 1]['amount']\n\n# Simulate 10,000 draws from the treatment distribution\nsimulated_treatment_draws = np.random.choice(treatment_amounts, size=10000, replace=True)\n\n# Display summary statistics of the simulated treatment draws\nprint(f\"Simulated Treatment Draws Summary:\")\nprint(f\"Mean: {np.mean(simulated_treatment_draws):.4f}\")\nprint(f\"Standard Deviation: {np.std(simulated_treatment_draws):.4f}\")\nprint(f\"Min: {np.min(simulated_treatment_draws):.4f}\")\nprint(f\"Max: {np.max(simulated_treatment_draws):.4f}\")\n\nSimulated Treatment Draws Summary:\nMean: 0.8452\nStandard Deviation: 8.2042\nMin: 0.0000\nMax: 400.0000\n\n\n\n# Simulate 100,000 draws from the control distribution\nsimulated_control_draws = np.random.choice(control_amounts, size=100000, replace=True)\n\n# Simulate 10,000 draws from the treatment distribution\nsimulated_treatment_draws = np.random.choice(treatment_amounts, size=10000, replace=True)\n\n# Calculate a vector of 10,000 differences\ndifferences = simulated_treatment_draws - simulated_control_draws[:10000]\n\n# Calculate the cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative Average of Differences\")\nplt.axhline(0, color='red', linestyle='--', label=\"Zero Line\")\nplt.xlabel(\"Number of Differences\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Cumulative Average of Differences Between Treatment and Control\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\n# Add a distribution curve to the plot\n# Fit a normal distribution to the differences\nmean_diff = np.mean(differences)\nstd_diff = np.std(differences)\n\n# Generate x values for the curve\nx = np.linspace(min(differences), max(differences), 1000)\ny = norm.pdf(x, mean_diff, std_diff)\n\n# Plot the distribution curve\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=\"Normal Distribution Curve\", color='purple')\nplt.hist(differences, bins=50, density=True, alpha=0.6, color='gray', edgecolor='black')\nplt.axvline(0, color='red', linestyle='--', label=\"Zero Line\")\nplt.xlabel(\"Differences\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of Differences with Fitted Curve\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\nimport numpy as np\n\n# Set the number of draws and repetitions\nsample_size = 50\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 50')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nAt a sample size of 50, the distribution of average differences is wide and variable. The mean is around 0.174, suggesting a positive treatment effect, but the high variance means zero still lies relatively close to the center. With this small sample, the result is noisy, and we cannot confidently reject the possibility that the true effect is zero.\n\n# Set the number of draws and repetitions\nsample_size = 200\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 200')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWhen the sample size increases to 200, the distribution tightens, and the average difference remains positive at about 0.191. Zero begins to move away from the center but still lies within a range where random variation could plausibly explain the observed differences.\n\n# Set the sample size and number of repetitions\nsample_size = 500\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 500')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nAt a sample size of 500, the distribution becomes more concentrated, and the mean difference of 0.156 shows less variability. Zero is no longer near the middle and starts to appear in the tails. This suggests a stronger signal from the treatment, with less influence from random noise.\n\n# Set the number of draws and repetitions\n# Set the sample size and number of repetitions\nsample_size = 1000\nnum_repetitions = 1000\n\n# Initialize a list to store the average differences\naverage_differences = []\n\n# Perform the simulation\nfor _ in range(num_repetitions):\n    # Take random draws from the control and treatment distributions\n    control_sample = np.random.choice(control_amounts, size=sample_size, replace=True)\n    treatment_sample = np.random.choice(treatment_amounts, size=sample_size, replace=True)\n    \n    # Calculate the average difference and store it\n    avg_diff = np.mean(treatment_sample) - np.mean(control_sample)\n    average_differences.append(avg_diff)\n\n# Plot the histogram of the average differences\nplt.figure(figsize=(10, 6))\nplt.hist(average_differences, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(np.mean(average_differences), color='red', linestyle='--', label=f'Mean: {np.mean(average_differences):.4f}')\nplt.xlabel('Average Difference')\nplt.ylabel('Frequency')\nplt.title('Histogram of Average Differences for a Sample Size of 1000')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nBy the time the sample size reaches 1000, the distribution is narrow and centered near 0.151. Zero is clearly in the tail, and the likelihood of observing such differences under the null is small. This provides strong evidence of a treatment effect.\nImportantly, zero moves from the center toward the tails of the distribution and eventually falls outside the typical range of differences we’d expect if there were no treatment effect.\nTogether, the histograms suggest that the treatment group performs better than the control group, and that this observed difference is unlikely to be due to random variation—especially at larger sample sizes. This supports the conclusion that the treatment has a real, measurable effect."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/homework1/index.html",
    "href": "projects/homework1/index.html",
    "title": "Homework 1",
    "section": "",
    "text": "Heading!\nSome text."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "anjanakhabir",
    "section": "",
    "text": "Welcome to my website!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]