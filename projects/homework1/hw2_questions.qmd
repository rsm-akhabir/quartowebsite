---
title: "Poisson Regression Examples"
author: "Anjana Khabir"
date: May 07, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
import pandas as pd
import seaborn as sns

# Load the Blueprinty data
data = pd.read_csv('/home/jovyan/Desktop/marketingwebsite/blueprinty.csv')

# Display the first few rows of the dataset
data.head()

```

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
palette = {0: "#2ca02c", 1: "#d62728"}  
sns.set(style="darkgrid") 
sns.histplot(data=data, x="patents", hue="iscustomer", bins=20, multiple="dodge", palette=palette)
plt.title("Number of Patents by Customer Status", fontsize=14)
plt.xlabel("Number of Patents", fontsize=12)
plt.ylabel("Number of Firms", fontsize=12)
plt.legend(title="Is Customer", labels=["Non-Customer", "Customer"], fontsize=10)
plt.tight_layout()
plt.show()
```

```{python}
# Filter data into customers and non-customers
customers = data[data['iscustomer'] == 1]
non_customers = data[data['iscustomer'] == 0]

# Calculate and print means
mean_customers = customers['patents'].mean()
mean_non_customers = non_customers['patents'].mean()
print(f"Mean number of patents for customers: {mean_customers}")
print(f"Mean number of patents for non-customers: {mean_non_customers}")
```

## Observations:
Companies that use Blueprinty typically hold a higher average number of patents (around 4.13) compared to those that don’t (approximately 3.47). The histogram reveals that customer firms are more frequently found among those with greater patent counts, hinting at a possible positive link between Blueprinty usage and patent performance. Nevertheless, this trend could also be shaped by additional variables like geographic location or the age of the firm, which warrant further investigation.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

palette = {0: "#2ca02c", 1: "#d62728"}
plt.figure(figsize=(8, 6))
sns.countplot(data=data, x="region", hue="iscustomer", palette=palette)
plt.title("Region Distribution by Customer Status")
plt.xlabel("Region")
plt.ylabel("Number of Firms")
plt.legend(title="Is Customer", labels=["Non-Customer", "Customer"])
plt.tight_layout()
plt.show()
```

```{python}
# Compare regions by customer status
region_counts_customers = customers['region'].value_counts(normalize=True)
region_counts_non_customers = non_customers['region'].value_counts(normalize=True)

print("Region distribution for customers:")
print(region_counts_customers)
print("\nRegion distribution for non-customers:")
print(region_counts_non_customers)

# Compare ages by customer status
mean_age_customers = customers['age'].mean()
mean_age_non_customers = non_customers['age'].mean()

print(f"\nMean age for customers: {mean_age_customers}")
print(f"Mean age for non-customers: {mean_age_non_customers}")

```

The geographic distribution of firms differs between Blueprinty customers and non-customers, with certain regions—such as the Northeast—showing a greater proportion of customer firms. This indicates that regional factors could potentially confound the observed relationship between software adoption and patent performance.

In terms of firm age, customers tend to be slightly older on average (approximately 26.9 years) compared to non-customers (about 26.1 years), though the difference is relatively small. Nonetheless, accounting for firm age remains important to ensure the analysis yields unbiased insights.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

```{python}
# Define the Poisson likelihood function
import numpy as np

def poisson_likelihood(lambda_, Y):
   """
   Compute the Poisson likelihood for given lambda and observed Y.
   """
   likelihood = np.exp(-lambda_) * (lambda_ ** Y) / np.math.factorial(Y)
   return likelihood

# Example usage
Y_observed = 5  # Example observed number of patents
lambda_example = 3.0  # Example lambda value
likelihood_value = poisson_likelihood(lambda_example, Y_observed)
print(f"Poisson likelihood for Y={Y_observed} and lambda={lambda_example}: {likelihood_value}")
```



```{python}
def poisson_loglikelihood(lambda_, Y):
   """
   Compute the Poisson log-likelihood for given lambda and observed Y.
   """
   from scipy.special import factorial
   log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - np.log(factorial(Y)))
   return log_likelihood
```



```{python}
# Define a range of lambda values
lambda_values = np.linspace(0.1, 10, 100)

# Convert Y_sample to a NumPy array
Y_sample = np.array([5, 3, 4, 6, 2])  # Example observed data

# Compute the log-likelihood for each lambda
log_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambda_values]

# Plot the log-likelihood
plt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')
plt.xlabel('Lambda')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood vs Lambda')
plt.axvline(x=np.mean(Y_sample), color='red', linestyle='--', label='MLE (Mean of Y)')
plt.legend()
plt.show()
```


```{python}
# Derive the first derivative of the log-likelihood function
# Log-likelihood: l(lambda) = Y * log(lambda) - lambda - log(Y!)
# First derivative: dl/dlambda = Y / lambda - 1

# Solve for lambda when dl/dlambda = 0
# Y / lambda - 1 = 0 => lambda = Y

# For a sample of observations, the MLE for lambda is the sample mean (Ybar)
def compute_lambda_mle(Y):
   """
   Compute the MLE for lambda (mean of Y).
   """
   return np.mean(Y)

# Example usage
Y_sample = [5, 3, 4, 6, 2]  # Example observed data
lambda_mle = compute_lambda_mle(Y_sample)
print(f"MLE for lambda (mean of Y): {lambda_mle}")
```


```{python}
from scipy.optimize import minimize

# Define the negative log-likelihood function for optimization
def negative_log_likelihood(lambda_, Y):
   return -poisson_loglikelihood(lambda_, Y)

# Example observed data
Y_sample = np.array([5, 3, 4, 6, 2])

# Initial guess for lambda
initial_guess = np.mean(Y_sample)

# Perform optimization to find the MLE
result = minimize(negative_log_likelihood, x0=initial_guess, args=(Y_sample,), bounds=[(0.01, None)])

# Extract the MLE for lambda
lambda_mle = result.x[0]
print(f"MLE for lambda: {lambda_mle}")
```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

# Compute the Poisson regression log-likelihood for given beta, observed Y, and covariate matrix X.


```{python}

import numpy as np
from scipy.special import gammaln

# Define Poisson regression log-likelihood function
def poisson_regression_loglikelihood(beta, Y, X):
    Xbeta = X @ beta
    lambdas = np.exp(Xbeta)
    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))

```


```{python}
import numpy as np
from scipy.optimize import minimize
from scipy.linalg import inv
from scipy import optimize
from scipy.special import gammaln

# Prepare the design matrix X
data['age_squared'] = data['age'] ** 2

region_dummies = pd.get_dummies(data['region'], drop_first=True)

# Construct design matrix
X = pd.concat([
    pd.Series(1, index=data.index, name="intercept"),
    data["age"],
    data["age_squared"],
    region_dummies,
    data["iscustomer"]
], axis=1)

Y = data["patents"].values
X_matrix = X.values
def poisson_regression_loglikelihood(beta, X, Y):
    beta = np.atleast_1d(np.asarray(beta))
    Xb = np.dot(X, beta).astype(np.float64)
    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent
    lam = np.exp(Xb_clipped)

    return np.sum(-lam + Y * Xb - gammaln(Y + 1))

def neg_loglike(beta, X, Y):
    return -poisson_regression_loglikelihood(beta, X, Y)


initial_beta = np.zeros(X.shape[1])
result = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')
beta_hat = result.x
hessian_inv = result.hess_inv
std_errs = np.sqrt(np.diag(hessian_inv))
summary = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": std_errs
}, index=X.columns)

summary
```

## Validate the results above using `statsmodels.GLM()` from Python

```{python}
import statsmodels.api as sm

# Drop 'intercept' column and ensure all data is float
X_glm = X.drop(columns='intercept', errors='ignore').astype(float)

# Add constant for intercept term
X_glm = sm.add_constant(X_glm)

# Fit GLM model
glm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())
glm_results = glm_model.fit()

# Display summary
glm_results.summary()

# Conduct coefficient summary and create a table
coefficient_summary = glm_results.summary2().tables[1][["Coef.", "Std.Err."]]

# Display table
coefficient_summary

```

## Interpretations:
- Older firms tend to have higher patent counts, indicating a strong positive relationship between age and patenting activity.

- The negative and significant coefficient on Age² suggests that the positive effect of age weakens over time—patent growth slows as firms mature.

- Firms using Blueprinty are predicted to have 23% more patents than similar non-users, based on a significant coefficient of 0.2076 (p < 0.001).

- Differences across regions (e.g., Northeast, Northwest) are not statistically significant, suggesting geography has little influence on patent outcomes once other factors are controlled.

- The effect of Blueprinty’s software is further assessed using counterfactual prediction to isolate its impact.

## The effect of Blueprinty’s software is further assessed using counterfactual prediction to isolate its impact.

We perform a counterfactual simulation by constructing two hypothetical scenarios:

X_0: All firms are assigned as non-customers (iscustomer = 0)

X_1: All firms are assigned as customers (iscustomer = 1)

Using the fitted model, we predict the expected number of patents for each firm under both scenarios. The difference in predicted outcomes gives us an estimate of the average effect of using Blueprinty’s software.

```{python}
# Create counterfactual datasets:
# X_0: simulate all firms as non-customers
# X_1: simulate all firms as customers
X_0 = X_glm.copy()
X_1 = X_glm.copy()

X_0["iscustomer"] = 0
X_1["iscustomer"] = 1

# Predict patent counts under both scenarios
y_pred_0 = glm_results.predict(X_0)
y_pred_1 = glm_results.predict(X_1)

print(y_pred_0)
print(y_pred_1)

# Calculate the average treatment effect
average_effect = np.mean(y_pred_1 - y_pred_0)

average_effect
```

The average predicted difference in patent counts between firms that use Blueprinty and those that do not is 0.793. This suggests that, after accounting for firm age and regional factors, Blueprinty customers are expected to file roughly 0.793 more patents over a five-year period than comparable non-customers.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


```{python}
# Load the Blueprinty data
airbnb = pd.read_csv('/home/jovyan/Desktop/marketingwebsite/airbnb.csv')

# Display the first few rows of the dataset
airbnb.head()
```

```{python}
print(airbnb.shape)
print(airbnb.info())
```


```{python}
# Show the number of missing values in each column
missing_values = airbnb.isnull().sum()
print("Number of missing values in each column:")
print(missing_values)
```

```{python}
airbnb.dropna(subset=['review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)
```


```{python}
airbnb.describe(include='object')
```

```{python}
airbnb['last_scraped'] = pd.to_datetime(airbnb['last_scraped'])
airbnb['host_since'] = pd.to_datetime(airbnb['host_since'])

airbnb['days'] = (airbnb['last_scraped'] - airbnb['host_since']).dt.days

```

Correlation Matrix for Numerical Variables
```{python}
numeric_cols = ['price', 'bathrooms', 'bedrooms', 'number_of_reviews', 
                'review_scores_cleanliness', 'review_scores_location', 
                'review_scores_value', 'days']
sns.heatmap(airbnb[numeric_cols].corr(), annot=True)

```

Price Distribution
```{python}
sns.histplot(airbnb['price'], bins=50)
```

Price vs. Room Type
```{python}
sns.boxplot(x='room_type', y='price', data=airbnb)
```

### Poisson Regression Model Using `statsmodels.GLM()`

```{python}
# Prepare the design matrix X and response variable Y
airbnb['room_type_encoded'] = airbnb['room_type'].astype('category').cat.codes
X_airbnb = airbnb[['bathrooms', 'bedrooms', 'number_of_reviews', 
                   'review_scores_cleanliness', 'review_scores_location', 
                   'review_scores_value', 'days', 'room_type_encoded']]

# Drop rows with NaN or infinite values
X_airbnb = X_airbnb.replace([np.inf, -np.inf], np.nan).dropna()
Y_airbnb = airbnb.loc[X_airbnb.index, 'price']  # Ensure Y matches filtered X

X_airbnb = sm.add_constant(X_airbnb)  # Add intercept term

# Fit Poisson regression model
poisson_model = sm.GLM(Y_airbnb, X_airbnb, family=sm.families.Poisson())
poisson_results = poisson_model.fit()

# Display summary of the model
poisson_results.summary()
```

## Interpretation
Intercept (3.4662): Represents the baseline log-expected value of the outcome when all other variables are set to zero.

Bathrooms (+0.3205): Listings with more bathrooms are associated with higher expected counts of the outcome. Each additional bathroom increases the expected count on the log scale.

Bedrooms (+0.1852): More bedrooms are linked to higher activity—each additional bedroom raises the expected log count.

Number of Reviews (-0.0001): Having more past reviews is slightly negatively associated with the expected count. The effect is small but statistically significant.

Cleanliness Score (+0.0122): Higher cleanliness ratings are associated with modest increases in the expected count of the outcome.

Location Score (+0.1820): Better-rated locations show a strong positive association with the expected outcome.

Value Score (-0.0785): Surprisingly, higher value ratings are linked to lower expected counts. This may suggest that "value" is higher in less competitive or less booked listings.

Days (+0.00003758): Listings that have been active longer tend to show slightly higher expected counts, reflecting accumulated exposure over time.

Room Type (Encoded) (-0.6924): The room type significantly influences outcomes. Encoded types (likely private/shared rooms) are associated with notably lower expected counts compared to the reference category.
